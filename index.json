[{"content":" The inner machinations of my mind are an enigma # ","date":"February 01 2026","externalUrl":null,"permalink":"/devblog/","section":"","summary":"","title":"","type":"page"},{"content":" All Posts :) # ","date":"February 01 2026","externalUrl":null,"permalink":"/devblog/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"For the\n","date":"February 01 2026","externalUrl":null,"permalink":"/devblog/posts/github-action--hugo/","section":"","summary":"","title":"Github Actions and HUGO for website themes with markdown files","type":"posts"},{"content":" Abstract # The idea is to create a script with streamlined order placement, management, portfolio performance tracking and organized data for technical analysis. This will be combined with my consolidated CSV files from Rocket Money to provide projections for investment rate and savings rate and spending information to create the ultimate finance tool.\nFeatures # Rocket Money CSV Files for Spending Information Identity Theft Monitoring ?? Plaid API? Portfolio Tracking and Analysis Projections on Saving, Retirement, and Investments Email Notifications Place and Manage Trade Orders through the IBKR API\nGetting the IBKR API up and running # To get the API up and running you will need to install either Trader Workstation (TWS) or IB Gateway.\nI have chosen to use the TWS API with a Python Client, since it is my most familiar language.\nI followed the Instructions at: https://www.interactivebrokers.com/campus/ibkr-api-page/twsapi-doc/ to properly configure TWS and the API\nAfter getting my paper trading account linked to practice with real data in a simulated environment\nCurrently In Progress # ","date":"January 28 2026","externalUrl":null,"permalink":"/devblog/posts/investment-tracking-and-management-with-ibkr-api/","section":"","summary":"","title":"Investment Tracking and Management with IBKR API","type":"posts"},{"content":"I\u0026rsquo;ve found myself feeling disconnected from current events for a few years now and I\u0026rsquo;ve been trapped in the loop of wanting to find news sources I can trust to read from regularly, but then find myself just skimming a hundred different sources because I\u0026rsquo;m not sure what to read.\nI thought using an API to request json object news articles would help me more effectively sift through the data to find what I want. I would prefer to consume a mix of financial, tech, science, and political news\nUsing this News API at: newsapi.org you can get a free API key in minutes\nNews API allows you to filter your results in a variety of ways, so I need to decide on the sources I\u0026rsquo;d like to scan articles from.\nThis News API will not produce results for searches that are too broad, and the criteria for \u0026rsquo;too broad\u0026rsquo; seems to be tied to the number of sources\nI first look through the different sources I have available:\ndef printSources() -\u0026gt; list: data = newsapi.get_sources() for source in data[\u0026#39;sources\u0026#39;]: print(source[\u0026#39;name\u0026#39;]) OUTPUT: ------------------------ ABC News ABC News (AU) Aftenposten Al Jazeera English ANSA.it Argaam Ars Technica Ary News Associated Press Australian Financial Review Axios BBC News... This returns a list of officially supported sources\nYou can also set specific domains to use as a filter in the get_everything() and get_top_headlines()\nSo I can either search by article author, source id, source name, source domain, and more to get more selective with my media.\nI decided on a small number of domains and keywords for daily searches\nI initially made a mistake in the logic of searching with keywords on each domain that resulted in the same article having duplicates.\nfor article in data[\u0026#39;articles\u0026#39;][:5]: if f\u0026#34;{article[\u0026#39;url\u0026#39;]}\\n\u0026#34; in usedlinks: pass else: email += f\u0026#34;\u0026#34;\u0026#34;====================\\nTitle: {article[\u0026#39;title\u0026#39;]}\\nAuthor: {article[\u0026#39;author\u0026#39;]}\\nDescription: {article[\u0026#39;description\u0026#39;][:80]}\\n{article[\u0026#39;url\u0026#39;]}====================\\n\u0026#34;\u0026#34;\u0026#34; print(article) links_just_used.append(article[\u0026#39;url\u0026#39;]) with open(\u0026#39;usedlinks.txt\u0026#39;, \u0026#39;a\u0026#39;) as file: for link in links_just_used: file.write(link + \u0026#34;\\n\u0026#34;) This was fixed by removing duplicates from the list before appending its data to the docstring. Since preserving the order of the list is not necessary then we can achieve this quickly by turning the list into a set and then back into a list.\nI also changed the data structure in the final version by adding unique articles to a dictionary and adding that dictionary to a list while in the loop. This removes the need to remove duplicates as duplicates would never make it past this formatting step.\nI request data on a daily basis at noon since more articles have been written later in the day, and the I send them to my own email in a fancy docstring with working hyperlinks through the python SMTP library\nThis works like a charm and the only way I see myself improving on this is by generating HTML \u0026amp; CSS and using an article image URL to make the email look more pretty, which I may do soon as it sounds cool.\n","date":"January 15 2026","externalUrl":null,"permalink":"/devblog/posts/news-curation-with-python-news-api/","section":"","summary":"","title":"News Curation with Python News API","type":"posts"},{"content":"Using Python and its integration with SQLCipher in sqlcipher3 I can create and manage encrypted tables of any information I want, namely credentials\nThis would streamline the local storage of credentials on my home machine without significant risk of compromise.\nThe syntax for interacting with sqlcipher is the same as a normal SQL table, with the addition of a new PRAGMA keyword to set and input the encryption key.\nThe first time a new table is made you will use a PRAGMA statement with \u0026lsquo;key = \u0026rsquo; to set the encryption key. It should be noted that you cannot use placeholders within PRAGMA declarations. This is because they are sort of like settings and not actual operations or queries so they don\u0026rsquo;t interact with the placeholders system of SQL. But this means the key, if accepted through user input has to be passed through an f-string, or raw text within the code. But a funny characteristic of PRAGMA makes it so that you cannot chain multiple commands with it, so it is not at risk of injection attack from user input.\nquery = \u0026#39;PRAGMA key={}\u0026#39; injection = \u0026#39;fakepasswordnonsense; FROM TABLE DROP *\u0026#39; passing the injection through this PRAGMA statement will result in an Operational Error caused by the inability to run multiple commands from PRAGMA.\nThe other points of interacting with our table are vulnerable to injection attack however, so steps will have to be made to make it resistant to them.\n1 being placeholders instead of f-strings.\nPlaceholders within SQL make it so that user-input is passed as separate arguments of a query rather than as raw text straight into the query. If I pass an injection attempt into an f-string query, it will be successful:\ni = input(\u0026#34;What account do you want to access\u0026#34;) Query = f\u0026#39;SELECT * FROM TABLE WHERE name = {i}\u0026#39; injection = \u0026#39;fakename; FROM TABLE _ DROP *\u0026#39; If i typed the injection as my input for I, everything within the selected table would be wiped out as I typed the command to drop the table and chained it with my response to the input request\nwith SQL placeholders this can\u0026rsquo;t happen so we will be using those instead.\nI was hesitant to add clipboard functionality since it would drastically decrease the security of this solution until I realized that manually keying them in also makes you vulnerable to a keylogger, so I\u0026rsquo;ve decided for a streamlined experience it would likely be worth it to add some sort of clipboard integration.\nThen it would not be out of the question for other people to find use in this product.\nHow to add control over the system clipboard with python? A simple lightweight clipboard module that runs on Pyperclip with the copy() function will suffice it works on MAC OS, Windows, and Linux\nimport clipboard text = \u0026#39;ddfewf122\u0026#39; clipboard.copy(text) print(clipboard.paste()) OUTPUT: ddfewf122 Adding a button on the main interface that copies the selected rows password will work sufficiently.\n","date":"January 06 2026","externalUrl":null,"permalink":"/devblog/posts/password-manager-with-sqlcipher/","section":"","summary":"","title":"Password Management with SQLCipher","type":"posts"},{"content":"Retrieval Augmented Generation or RAG, is a technology developed to further the capabilities of LLM\u0026rsquo;s by giving them access to a database full of context that they can use to formulate responses.\nYou could effectively expand what the LLM \u0026lsquo;knows\u0026rsquo; and give it topic specific information to better answer questions of that topic\nThis is achieved by taking large amounts of data that you want your Model to use and embedding them, or transposing them into vector space\nEmbedding can be complicated, but the basics are as follows. An embedding model starts with a large column of keywords or statuses a word could have, so something like:\nis_noun, is_alive, is_occupation, is_male Then, each keyword is weighted by how much it relates to the word you are embedding. So for a test word like \u0026lsquo;king\u0026rsquo; with the keywords above:\n0, 0.99, 0.7, 0.99 To accurately embed a word you will need very many of these keywords\nThen you are left with the embedding of each individual word, but how does that reflect the meaning of an entire sentence? Good question! It doesn\u0026rsquo;t.\nYou first need a way to reflect the context of each word in the sentence, and then aggregate them. Models like BERT are used for this by generating different embeddings for a word based on its surrounding words, then these are \u0026lsquo;pooled\u0026rsquo; together to create the final sentence embedding. This process can be repeated for larger scopes but can also lose accuracy. Each embedding is stored in a Vector Database for later reference.\nThe exact math behind this process involves multi-variable calculus and statistics, but after all of this you now have a database full of text that has its semantic and contextually meaning graphed into an N-dimensional vector space where N is the number of keywords mentioned earlier\nNow how do we use this to augment an LLM?\nBefore sending a query to your model, first embed it with the same function we used to embed our vector database earlier (this part is important). Then, compare that embedding with our vector database and pick which parts are the most similar.\nThis is typically done via cosine similarity, a function that calculates how \u0026lsquo;similar\u0026rsquo; two vectors are by the angle that separates them in space\nThe reason that functions like Euclidean Distance cannot be used for this is due to a phenomenon known as the \u0026lsquo;Curse of Dimensionality\u0026rsquo;, and is also the reason why we can\u0026rsquo;t increase the accuracy of embeddings just by adding millions of qualifying keywords.\nA vector database that was produced by a keyword set 784 words long (each word gets 784 other \u0026lsquo;words\u0026rsquo; to describe its meaning) has 784 dimensions in its output vector space, and 784-D space is calculator breaking huge. As the number of dimensions goes up: 1) the data stored within the space gets more and more sparse, and 2) the average distance between points approaches the maximum distance\nEssentially, it becomes impossible to graph how similar the embeddings are to each other anymore because they are too far apart\nThen we can send the extra data we retrieved as context for a model to answer the query. You can literally pass it directly into a query with something like:\n\u0026quot;Answer this question '{query}' Using the following context: {context}\u0026quot; The reason that the function you use to embed your database and your queries must be the same is because of the differences in output between different functions. There are a lot of different sophisticated ways to graph words into high dimensional vector space and these different methods or keyword sets produce slightly different embeddings. So using different methods at different links of the chain makes it impossible to relate semantic meaning between two embeddings of different vector spaces\nThis process alone can still lack accuracy with fetching data related to your query, especially in larger databases. There are other ways to refine the accuracy of our retrieval, such as Re-ranking.\nRe-ranking is the process of comparing the data retrieved from the vector database back to the original query.\nDuring the initial retrieval process you were calculating the similarity between potentially thousands of vectors, and say you wanted to feed your model the top 5 most similar passages to your query. You would just retrieve the the top 5 and feed it to the model.\nWith Re-ranking we would first retrieve a larger number of similar data like the top 15, and then use those 15 as a second more precise vector database for another retrieval.\nThat is the quickest and easiest way to implement a re-ranker and there are obviously 50 sophisticated different ways to increase the accuracy of an LLM, but this solution above could be implemented without much code or effort as the tools to embed and store vector databases exist within LangChain and can be used with free locally-run LLM\u0026rsquo;s like Ollama\u0026rsquo;s 3.1:8b\n","date":"January 02 2026","externalUrl":null,"permalink":"/devblog/posts/rag-with-ollama--langchain/","section":"","summary":"","title":"RAG with Ollama \u0026 LangChain","type":"posts"},{"content":" A little about me # I work as Starbucks barista and I enjoy programming on the side :) # This is where I post the fruits of my labor, along with my GitHub account at: # github.com/geohadg # I Program mostly in Python, have dabbled in Terraform, PowerShell Core, Node.js, JavaScript, and am currently taking a class on C++ # ","externalUrl":null,"permalink":"/devblog/about/","section":"","summary":"","title":"","type":"about"},{"content":"All of my projects in the Conceptual, Development, and Refinement stages\nThis is also where the official documentation will reside, different updates will be posted on the \u0026lsquo;posts\u0026rsquo; page\n","externalUrl":null,"permalink":"/devblog/projects/","section":"","summary":"","title":"","type":"projects"},{"content":"","externalUrl":null,"permalink":"/devblog/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/devblog/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/devblog/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/devblog/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Welcome to my development blog! # This is where I will be sharing my projects, and posting about the numerous challenges I have run into throughout my journey and how I overcame them (hint: alot of doc stalking and headache) # I thoroughly enjoy the art of development and I wish to share that joy and hopefully create some useful tools for others # ","externalUrl":null,"permalink":"/devblog/posts/first_article/","section":"","summary":"","title":"Welcome! First Post","type":"posts"}]